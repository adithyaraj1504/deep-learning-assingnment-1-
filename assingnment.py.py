# -*- coding: utf-8 -*-
"""deep learning assingnment 1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_tx7as35Ttb8wOWfrve34NtVOMTdqsPy
"""

import pandas as pd

df1 = pd.read_csv('mnist_train.csv')



df2 = pd.read_csv('mnist_test.csv')

combined_df = pd.concat([df1, df2], ignore_index=True)

 # In kaggle.com a single data set is not given instead a test and train dataset is given seperately and to get the complete dataset we combine both the dataset
combined_df.to_csv('combined_file.csv', index=False)

print("Combined CSV file saved successfully!")

import pandas as pd

# Read the CSV file into a DataFrame
df = pd.read_csv('combined_file.csv')

# Displaying the combined dataset
print(df)

X = df.loc[:,df.columns!='label'].to_numpy()       # taking all the coloumns apart from the label as the input parameters
y = df['label'].to_numpy()                         # taking the label as the output of the classification

import numpy as np
import matplotlib.pyplot as plt
                                                           # Since the first question is to display images we convert the pixels to image by reshaping
def construct_image(pixel_list):
  image_array = np.array(pixel_list).reshape(28,28)
  plt.imshow(image_array,cmap = 'gray')
  plt.axis('off')
  plt.show

print("The below image is ",y[0])                         # printing the first image
construct_image(X[0])

print("The below image is ",y[1])                # printing the second image
construct_image(X[1])

print("The below image is ",y[2])
construct_image(X[2])                                            # printing the third image

print("The below image is ",y[3])
construct_image(X[3])                                # printing the fourth image

print("The below image is ",y[4])
construct_image(X[4])                              # printing the fifth image

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)       # having 20% of the data in the train section

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier                           # all the necessary libraries and function for building the model
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import confusion_matrix ,ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier

n_neighbors = 5

classifiers = {
    "Naive Bayes": GaussianNB(),                                       # having the model naive bayes , decision tree , random forest and KNN( nighbours =5) in the form of a dictionary
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    f"k-Nearest Neighbors (k={n_neighbors})": KNeighborsClassifier(n_neighbors=n_neighbors)
}

for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)                                          # fitting the model
    predictions = classifier.predict(X_test)                                  # finding the predictions
    accuracy = accuracy_score(y_test, predictions)                            # finding the accuracy of the model
    precision = precision_score(y_test, predictions, average='weighted')      # finding the precision of the model
    recall = recall_score(y_test, predictions, average='weighted')            # finding the recall of the model
    f1 = f1_score(y_test, predictions, average='weighted')                    # finding the f1 score of the model
    cm = confusion_matrix(y_test,predictions)                                 # calculating the confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix = cm)
    disp.plot()                                                               # plotting the confusion matrix

    print("Classifier:", name)
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("\n")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)  # Now training and testing ratio is 50:50

for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    recall = recall_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')                         # same explanation as above
    cm = confusion_matrix(y_test,predictions)
    disp = ConfusionMatrixDisplay(confusion_matrix = cm)
    disp.plot()

    print("Classifier:", name)
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("\n")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)     # training and testing is 20 : 80

for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    recall = recall_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')                          # same explanation as above
    cm = confusion_matrix(y_test,predictions)
    disp = ConfusionMatrixDisplay(confusion_matrix = cm)
    disp.plot()

    print("Classifier:", name)
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("\n")

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42)        # testing and training are in the ratio 1:99

for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    recall = recall_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')                              # same explanation as above
    cm = confusion_matrix(y_test,predictions)
    disp = ConfusionMatrixDisplay(confusion_matrix = cm)
    disp.plot()

    print("Classifier:", name)
    print("Accuracy:", accuracy)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)
    print("\n")

"""when the test train split is 80:20 then the model with the highest accuracy is KNN ( n =5) with an accuracy of 97%

when the test train split is 50:50 then the model with the highest accuracy is
KNN(n=5) with an accuracy of 96.6%

when the test train split is 20:80 then the model with the highest accuracy is
Random forest of accuracy 95.3%

when the test train split is 1:99 then the model with the highest accuracy is
Random forest of accuracy 87%


Some from the above observation both  random forest and KNN(n=5) are best models for this classification

Novelty : Implementing all the method name and its function in the form of a dictionary and finding the accuracy of all the models with a single for loop
"""